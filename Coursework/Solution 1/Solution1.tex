\documentclass[a4paper,11pt]{article}
\usepackage{ml}
\usepackage{mlsubmit}
\newcommand\tab[1][1cm]{\hspace*{#1}}
\begin{document}

\initmlsubmision{1} % assignment number
{Lt Commander Rahul Raj}   % your name
{18111053}	% your roll number

\begin{mlsolution}
\section{Sub Question (1)  Misclassification Rate	}
\tab Classification Error is one of the methods to measure the purity or homogenity of set of datapoints at a Node in Decision Tree. It is calculated as below,
$$Err(Node) = 1 - \max\limits_{c\  \epsilon\  C} [P_c] $$

\tab \tab \tab where $P_c$ is the Probability of the Class 'c' in Node\\

\noindent
\tab Calculating misclassification error of Parent Node of given Tree A: -
\begin{align*}
  Err(A_{Parent}) &= 1 - \max\ [P(A_{Parent})]\\
  &= 1 - \max(P(A_{Left}),P(A_{Right}))\\
  &= 1 - \max(P(400),P(400))\\
  &= 1 - \frac{1}{2}\\
  &= 0.5
\end{align*}

\noindent
\tab Similarly for  Left Node and Right Node of Tree A\\
\begin{equation*}
  \begin{split}
    Err(A_{Left}) &= 1 - \max\ [P(A_{Left})]\\
    &= 1 - \max\ (P(0), P(1))\\
    &= 1 - \frac{3}{4}\\
    &= 0.25
  \end{split}
\quad\linebreak\quad
  \begin{split}
    Err(A_{Right}) &= 1 - \max\ [P(A_{Right})]\\
    &= 1 - \max\ (P(0), P(1))\\
    &= 1 - \frac{3}{4}\\
    &= 0.25
  \end{split}
\end{equation*}
\noindent
\tab The mislassification rate of \textbf{Tree A} is calculated as\\
\\
\tab \textbf{Misclassification Rate} $= (P(A_{Left}) \times Err(A_{Left}) + P(A_{Right}) \times Err(A_{Right}))$
\begin{align*}
  &=(\frac{400}{800} \times 0.25) + (\frac{400}{800} \times 0.25)\\
  &= \textbf{0.25}\ or\ \textbf{25\%}
\end{align*}
\noindent
\tab Similarly, the mislassification rate of \textbf{Tree B} is calculated as\\
\\
\tab \textbf{Misclassification Rate} $= (P(B_{Left}) \times Err(B_{Left}) + P(B_{Right}) \times Err(B_{Right}))$
\begin{align*}
  &=(\frac{600}{800} \times \frac{1}{3}) + (\frac{200}{800} \times 0)\\
  &= \textbf{0.25}\ or\ \textbf{25\%}
\end{align*}
\subsection{Observations}
\tab From the above calculations, it is observed that the misclassificaiton rates of both Tree A and Tree B are same i.e 25\%.
\subsection{Conclusion}
\tab Growing the decision tree further will not yeild any good in improving/ minimizing the error rate.
\section{Sub Question (2)  Information Gain	}
\tab Entropy is another way to measure the purity or homogenity of set of datapoints. High Entropy means less purity. Entropy is calculated as follows,
$$H(S)\ =\ -\sum\limits_{c\  \epsilon\  C} {P_c \times Log_2 P_c}$$
\noindent
\tab \tab \tab \tab where $P_c$ is the probability of Class C\\\\
\noindent
\tab Information Gain of a node is the difference between Entropy before and after splitting datapoints of that node. In order to calculate the IG, the Entropy of Tree A need to be calculated as follows,
\begin{align*}
  H(A_{Parent}) &= \ -\sum\limits_{c\  \epsilon\  C} {P_c \times Log_2 P_c}\\
  &= -\ (P(A_{Left}) \times\ Log_2\ P(A_{Left})\ +\ P(A_{Right}) \times\ Log_2\ P(A_{Right}))\\
  &= -\ (\frac{1}{2} \times Log\ \frac{1}{2} + \frac{1}{2} \times Log\ \frac{1}{2} )\\
  &= 1
\end{align*}

\noindent
\tab Similarly for  Left Node and Right Node of Tree A\\
\begin{equation*}
  \begin{split}
    H(A_{Left}) &= \ -\sum\limits_{c\  \epsilon\  C}  P(A_{Left}) {\times Log_2 P(A_{Left})}\\
    &= -\ (\frac{3}{4} \times Log\ \frac{3}{4} + \frac{1}{4} \times Log\ \frac{1}{4} )\\
    &= 0.81125
  \end{split}
\quad\linebreak\quad
  \begin{split}
    H(A_{Right}) &= \ -\sum\limits_{c\  \epsilon\  C}  P(A_{Right}) {\times Log_2 P(A_{Right})}\\
    &= -\ (\frac{1}{4} \times Log\ \frac{1}{4} + \frac{3}{4} \times Log\ \frac{3}{4} )\\
    &= 0.81125
  \end{split}
\end{equation*}
\tab The Information Gain (IG$_A$) of \textbf{Tree A} calculated as\\
\\
\tab \textbf{Information Gain (IG$_A$)} $= H(s)\ -\sum\limits_{c\  \epsilon\  C}  \frac{|H(s_c)|}{|H(s)|}) {\times H(S_c)}$
\begin{align*}
  &=1-((\frac{400}{800} \times 0.81125) + (\frac{400}{800} \times 0.81125))\\
  &= \textbf{0.18875}
\end{align*}
\noindent
\tab Similarly, Information Gain (IG$_B$) of \textbf{Tree B} is calculated as\\
\\
\tab \textbf{Information Gain (IG$_B$)} $= H(s)\ -\sum\limits_{c\  \epsilon\  C}  \frac{|H(s_c)|}{|H(s)|}) {\times H(S_c)}$
\begin{align*}
  &=0.81125-((\frac{600}{800} \times 0.917628) + (\frac{200}{800} \times 0))\\
  &= \textbf{0.1230}
\end{align*}
\subsection{Observations}
\tab From the above calculations, it is seen that the IG of Tree A is higher than the IG of Tree B.  Information Gain is an indicator on change in purity of dataset while undertaking split at that point. More the IG, more the pure the dataset  becomes.
\subsection{Conclusion}
\tab Since IG$_A$ \textgreater\ IG$_B$, we are preferring Tree A over Tree B for further growing the Decision Tree.
\section{Sub Question (3) Different Answers for Sub Questions (1) and (2)}
\subsection{Observations}
\tab From the above, it is seen that the misclassification rate is same for Tree A and Tree B. This implies there is no advantage in further growing Decision Tree as there is no improvement in splitting either Tree A or Tree B.
\noindent\\
\\
\tab However, while calculating the Entropy, we observed that the Entropy after split in both the cases have come down in both Tree A and Tree B. and the larger reduction happened in case of Tree A. This means that the Decision Tree can be further grown with Tree A.
\subsection{Conclusion}
\tab If we are considering Misclassification Error at various nodes, and at some stage the same becomes equal, further analysis should be done with the help of Entropy method to find the further scope of growing the Decision Tree.

\end{mlsolution}

\begin{mlsolution}


\end{mlsolution}

\begin{mlsolution}

My solution to problem 3

\end{mlsolution}

\begin{mlsolution}

My solution to problem 4

\end{mlsolution}

\begin{mlsolution}

My solution to problem 5

\end{mlsolution}

\begin{mlsolution}

My solution to problem 6

\end{mlsolution}


\end{document}
